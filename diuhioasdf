[1mdiff --git a/processimage2.py b/processimage2.py[m
[1mindex e17b6af..c68d3f5 100644[m
[1m--- a/processimage2.py[m
[1m+++ b/processimage2.py[m
[36m@@ -24,11 +24,10 @@[m [mfrom threading import Thread[m
 import math[m
 import os[m
 import pickle[m
[31m-from camera_api import ENDPOINT[m
 import json[m
 [m
 # Keras uses Tensorflow Backend[m
[31m-sess = tf.Session()[m
[32m+[m[32msess = tf.compat.v1.Session()[m
 set_session(sess)[m
 [m
 current_directory = os.path.dirname(__file__)[m
[36m@@ -101,7 +100,9 @@[m [mclass ProcessImage(socketio.Client):[m
         # schedule to check if data is recieved[m
         # if data is not recieved anymore so proceed to terminate the process[m
         schedule.every(15).seconds.do(self.checkIfAlive)[m
[31m-        schedule.every(2).seconds.do(self.update_obd_data)[m
[32m+[m[32m        if not self.TEST_MODE: # update trip data to database if not test mode[m
[32m+[m[32m            schedule.every(2).seconds.do(self.update_obd_data)[m
[32m+[m
         @self.event[m
         def connect():[m
             print("Connected to server...")[m
[1mdiff --git a/test_tf.py b/test_tf.py[m
[1mindex cca2236..c3b792d 100644[m
[1m--- a/test_tf.py[m
[1m+++ b/test_tf.py[m
[36m@@ -1,3 +1,82 @@[m
[32m+[m[32mimport sys[m
[32m+[m[32mimport time[m
[32m+[m[32mimport numpy as np[m
 import tensorflow as tf[m
[32m+[m[32mimport cv2[m
 [m
[31m-def load_pb[m
[32m+[m[32m# Path to frozen detection graph. This is the actual model that is used for the object detection.[m
[32m+[m[32mPATH_TO_CKPT = 'opencv_face_detector_uint8.pb'[m
[32m+[m[32m# List of the strings that is used to add correct label for each box.[m
[32m+[m[32mPATH_TO_LABELS = 'opencv_face_detector.pbtxt'[m
[32m+[m[32mNUM_CLASSES = 2[m
[32m+[m[32mclass TensoflowFaceDector(object):[m
[32m+[m[32m    def __init__(self, PATH_TO_CKPT):[m
[32m+[m[32m        """Tensorflow detector[m
[32m+[m[32m        """[m
[32m+[m
[32m+[m[32m        self.detection_graph = tf.Graph()[m
[32m+[m[32m        with self.detection_graph.as_default():[m
[32m+[m[32m            od_graph_def = tf.compat.v1.GraphDef()[m
[32m+[m[32m            with tf.compat.v1.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:[m
[32m+[m[32m                serialized_graph = fid.read()[m
[32m+[m[32m                od_graph_def.ParseFromString(serialized_graph)[m
[32m+[m[32m                tf.import_graph_def(od_graph_def, name='')[m
[32m+[m[32m            print(tf.contrib.graph_editor.get_tensors(tf.get_default_graph()))[m
[32m+[m
[32m+[m[32m        with self.detection_graph.as_default():[m
[32m+[m[32m            config = tf.compat.v1.ConfigProto()[m
[32m+[m[32m            config.gpu_options.allow_growth = True[m
[32m+[m[32m            self.sess = tf.compat.v1.Session(graph=self.detection_graph, config=config)[m
[32m+[m[32m            self.windowNotSet = True[m
[32m+[m
[32m+[m
[32m+[m
[32m+[m[32m    def run(self, image):[m
[32m+[m[32m        """image: bgr image[m
[32m+[m[32m        return (boxes, scores, classes, num_detections)[m
[32m+[m[32m        """[m
[32m+[m
[32m+[m[32m        image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)[m
[32m+[m
[32m+[m[32m        # the array based representation of the image will be used later in order to prepare the[m
[32m+[m[32m        # result image with boxes and labels on it.[m
[32m+[m[32m        # Expand dimensions since the model expects images to have shape: [1, None, None, 3][m
[32m+[m[32m        image_np_expanded = np.expand_dims(image_np, axis=0)[m
[32m+[m[32m        image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')[m
[32m+[m[32m        # Each box represents a part of the image where a particular object was detected.[m
[32m+[m[32m        boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')[m
[32m+[m[32m        # Each score represent how level of confidence for each of the objects.[m
[32m+[m[32m        # Score is shown on the result image, together with the class label.[m
[32m+[m[32m        scores = self.detection_graph.get_tensor_by_name('detection_scores:0')[m
[32m+[m[32m        classes = self.detection_graph.get_tensor_by_name('detection_classes:0')[m
[32m+[m[32m        num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')[m
[32m+[m[32m        # Actual detection.[m
[32m+[m[32m        start_time = time.time()[m
[32m+[m[32m        (boxes, scores, classes, num_detections) = self.sess.run([m
[32m+[m[32m            [boxes, scores, classes, num_detections],[m
[32m+[m[32m            feed_dict={image_tensor: image_np_expanded})[m
[32m+[m[32m        elapsed_time = time.time() - start_time[m
[32m+[m[32m        print('inference time cost: {}'.format(elapsed_time))[m
[32m+[m
[32m+[m[32m        return (boxes, scores, classes, num_detections)[m
[32m+[m
[32m+[m
[32m+[m[32mif __name__ == "__main__":[m
[32m+[m[32m    TEST_VIDEO = "test_vdo.mp4"[m
[32m+[m[32m    tDetector = TensoflowFaceDector(PATH_TO_CKPT)[m
[32m+[m[32m    cap = cv2.VideoCapture(TEST_VIDEO)[m
[32m+[m[32m    windowNotSet = True[m
[32m+[m[32m    while True:[m
[32m+[m[32m        ret, image = cap.read()[m
[32m+[m[32m        if ret == 0:[m
[32m+[m[32m            break[m
[32m+[m
[32m+[m[32m        [h, w] = image.shape[:2][m
[32m+[m[32m        print (h, w)[m
[32m+[m[32m        image = cv2.flip(image, 1)[m
[32m+[m
[32m+[m[32m        (boxes, scores, classes, num_detections) = tDetector.run(image)[m
[32m+[m
[32m+[m
[32m+[m
[32m+[m[32m    cap.release()[m
